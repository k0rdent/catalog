tags:
  - AI/Machine Learning
title: "Open WebUI"
summary: "A User-Friendly Web Interface for Chat Interactions."
logo: "https://raw.githubusercontent.com/open-webui/open-webui/main/static/favicon.png"
description: |
    Open WebUI is an extensible, feature-rich, and user-friendly self-hosted AI platform designed to operate entirely offline.
    It supports various LLM runners like Ollama and OpenAI-compatible APIs, with built-in inference engine for RAG, making it
    a powerful AI deployment solution.

    <div>
    ![](https://docs.openwebui.com/assets/images/demo-d3952c8561c4808c1d447fc061c71174.gif){ width="700" }
    </div>
install_code: |
    ~~~bash
    helm install open-webui oci://ghcr.io/k0rdent/catalog/charts/open-webui-service-template \
      --version 5.20.0 -n kcm-system
    ~~~
verify_code: |
    ~~~bash
    kubectl get servicetemplates -A
    # NAMESPACE    NAME                       VALID
    # kcm-system   open-webui-5-20-0          true
    ~~~
deploy_code: |
    ##### Configuration without GPU
    Tested on worker `instanceType: t3.xlarge` and `rootVolumeSize: 32`:
    ~~~yaml
    apiVersion: k0rdent.mirantis.com/v1alpha1
    kind: ClusterDeployment
    metadata:
      name: aws-example
    spec:
      template: aws-standalone-cp-{{ dash_version }}
      credential: aws-credential
      config:
        ...
        worker:
          instanceType: t3.xlarge
          rootVolumeSize: 32
        workersNumber: 1
    ~~~

    Tested service configuration:
    ~~~yaml
    apiVersion: k0rdent.mirantis.com/v1alpha1
    kind: MultiClusterService
    metadata:
      name: open-webui
    spec:
      clusterSelector:
        matchLabels:
          group: demo
      serviceSpec:
        services:
        - template: open-webui-5-20-0
          name: open-webui
          namespace: open-webui
          values: |
            open-webui:
              ollama:
                ollama:
                  models:
                    pull: [smollm:135m]
                    run: [smollm:135m]
              ingress:
                enabled: true
                class: "nginx"
                host: 'openwebui.example.com'
    ~~~
    
    ##### Configuration with GPU
    This setup requires corresponding cluster setup, see [NVIDIA GPU Operator](../../../apps/nvidia/nvidia/#__tabbed_1_2){ target="_blank" }

    ~~~yaml
    apiVersion: k0rdent.mirantis.com/v1alpha1
    kind: MultiClusterService
    metadata:
      name: open-webui
    spec:
      clusterSelector:
        matchLabels:
          group: demo
      serviceSpec:
        services:
        - template: open-webui-5-20-0
          name: open-webui
          namespace: open-webui
          values: |
            open-webui:
              ollama:
                ollama:
                  gpu:
                    enabled: true
                    type: 'nvidia'
                    number: 1
                  models:
                    pull: [llama3.2:3b]
                    run: [llama3.2:3b]
              ingress:
                enabled: true
                class: "nginx"
                host: 'openwebui.example.com'
    ~~~
doc_link: https://docs.openwebui.com/
use_ingress: true
