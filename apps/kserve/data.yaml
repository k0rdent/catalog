tags:
  - AI/Machine Learning
  - Serverless
title: Kubeflow KServe
summary: "Serverless platform for deploying, monitoring, and managing ML models on Kubernetes."
logo: ./assets/kserve_logo_icon.svg # https://github.com/kserve/artwork/blob/main/icon/color/k-serve-icon-color.svg
logo_big: ./assets/kserve_logo.svg # https://github.com/kserve/artwork/blob/main/color/k-serve-color.svg
created: "2025-04-22T06:02:26Z"
description: |
    k0rdent facilitates the deployment and lifecycle management of [KServe](https://kserve.github.io/website/latest/){ target="_blank" }
    within Kubernetes clusters by leveraging its multi-cluster orchestration capabilities.
    KServe provides a Kubernetes-native platform for serving and managing machine learning models with high performance and scalability.
    
    KServe builds on Kubernetes to provide a standardized, scalable, and reliable solution for deploying, monitoring, and managing 
    machine learning models in production. With features like traffic management, canary deployments, and autoscaling, KServe enables 
    seamless deployment of models across various frameworks including [TensorFlow](https://www.tensorflow.org/){ target="_blank" },
    [PyTorch](https://pytorch.org/){ target="_blank" }, [scikit-learn](https://scikit-learn.org/){ target="_blank" },
    [XGBoost](https://xgboost.readthedocs.io/){ target="_blank" }, and [ONNX](https://onnx.ai/){ target="_blank" }.
    
    KServe consists of two main components:

    1. **KServe CRDs** - Custom Resource Definitions that provide the foundation for KServe's functionality
    2. **KServe Controller** - The controller that implements serverless model serving capabilities
    
    Through k0rdent, users can easily install and manage KServe across multiple clusters, providing a centralized control plane
    to monitor health, enforce policies, and enable enterprise-grade machine learning serving. This application provides the ability to deploy
    the KServe Controller in Serverless mode without ModelMesh functionality enabled, which is optimized for environments 
    where autoscaling and scale-to-zero capabilities are prioritized.
    
    By deploying KServe from the catalog, k0rdent enables users to build and scale ML inference services while leveraging Kubernetes-native infrastructure.
# support_link:
charts:
  - name: kserve-crd
    versions: ['v0.15.0']
  - name: kserve
    versions: ['v0.15.0']

doc_link: https://kserve.github.io/website/master/get_started/

# test settings
test_deploy_chart: false
test_wait_for_pods: "kserve-controller-manager-"

examples:
  with_istio:
    title: With Istio
    chart_folder: example_istio

validated_amd64: 'y'
validated_aws: 'y'
validated_azure: 'y'
validated_arm64: 'y' # t4g.2xlarge
validated_local: 'y'
